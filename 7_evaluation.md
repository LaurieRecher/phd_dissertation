# 7. Evaluations visant à estimer la capacité d'Innodata à faciliter les interactions entre les opérateurs de l'open data et les *data spaces*

Dans le chapitre précédent, nous avons détaillé le  [**processus de prototypage**](README.md) qui nous a amené à concevoir l'[**artefact**](README.md). Aussi avons-nous décrit formellement la structure de cet artefact et expliqué en quoi ses différents éléments pourraient répondre aux différents aspects de la question. Ces explications sont autant de réponses temporaires dont il s'agit d'évaluer la [**pertinence empirique**](README.md) et la [**rigueur scientifique**](README.md) dans ce chapitre. 

Nous avons mené des évaluations dans le but d'estimer successivement la capacité des différentes versions d'Innodata (i.e les artefacts) à conduire l'innovation de services issue de l'open data dans les villes intelligentes vers l'idéal qu'elles se sont fixées. En effet, le choix de la *design science research methodology* suppose d'adopter une démarche intrinséquement itérative dans laquelle une première réponse artefactuelle est apportée avant d'être évaluée. Si la réponse est suffisamment pertinente empiriquement et rigoureuse scientifiquement, alors la recherche s'arrête. Dans le cas contraire, la question de recherche est précisée et un nouveau cycle de recherche démarre. Cette nouvelle itération apporte une deuxième réponse artefactuelle qui est à son tour évaluée, et ainsi de suite.

**Ajouter évaluation du process (VS. évaluation du produit) ?**
- PhD Working Book (4) - p.-81 - Retour comité ICO - chercher le problème plutôt que la solution
- PhD Working Book (5) - p.26-27 - Pattern d'alignement avec un paradigme - Insérer les itérations dans la littérature scientifique
- PhD Working Book (5) - p.71-74 - Validation de la méthodo au RGCS (Barcelone)
- PhD Working Book (5) p.111-113 - Feedback RGCS sur méthodo
- PhD Working Book (7) p.72-75 - Construction de l'évaluation process auprès AIM Nantes
- PhD Working Book (7) p.88 - Feedback évaluation process - Consortium doctoral AIM Nantes
- PhD Working Book (7) - p.98 - Feedback sur la nature des kernel theories à solliciter






- [x] Décrire la structure générale des évaluations menées à la fin de chaque cycle de recherche. 
- [x] Expliquer le lien avec la partie prototypage et avec la première étape du nouveau cycle 
- [x] Dédoubler en évaluation de la pertinence empirique et de la rigueur scientifique 

La stratégie globale d'évaluation des artefacts que nous avons mis en place (et qui s'est décliné en fonction des spécificités de chaque itération) reprend les travaux de Venable et al. (2012). Ils proposent un processus de décision éprouvé qui permet d'établir une stratégie d'évaluation d'artefact. Nous l'avons appliqué à chaque itération. Nous aurions pu également nous baser sur les travaux de Pries-Heje (2008). Or, ces derniers proposent un processus de décision moins exhaustif et des stratégies d'évaluation pour un nombre restreint de types de projets de recherche en *design science*. Le processus de Venable et al. (2012) est dès lors plus adaptable aux spécificités de cette thèse. 

### Première étape de construction de la stratégie d'évaluation : analyse de l'évaluation
La première étape du processus de décision consiste, une fois les activités de prototypage terminées, à évaluer le contexte et les objectifs de la phase d'évaluation. Venable et al. (2012) préconise de déterminer les 7 critères suivants et leur importance relative : 

1. [**La forme de l'artefact à évaluer**](README.md) : s'agit-il d'un [**concept**](README.md) ? d'un [**modèle**](README.md) ? d'une [**méthode**](README.md) ? d'une [**instanciation**](README.md) ? d'une [_**design theory**_](README.md) ?

2.  [**La nature de l'artefact à évaluer**](README.md). On distingue généralement les artefacts qui prennent la forme d'un [**produit**](README.md) et ceux qui prennent la forme d'un process. Les [**process**](README.md) se distinguent des produits en ce qu'ils ajoutent aux aspects techniques d'un artefact (technologie utilisée, architecture matérielle ou logicielle) une dimension socio-technique dans la mesure où une interaction avec les humains est nécessaire pour donner à l'artefact toute son utilité. 

3. [**Les propriétés de l'artefact à évaluer**](README.md) : s'agit-il d'évaluer l'[**utilité**](README.md) ? l'[**efficience**](README.md) ? l'[**éfficacité**](README.md) ? la [**qualité**](README.md) ?

4. **Les objectifs de l'évaluation** : s'agit-il d'évaluer si l'artefact permet d'atteindre des objectifs fixés au préalable ? de le comparer à d'autres artefacts ? d'évaluer les effets collatéraux ou les conséquences inattendues de l'implantation de l'artefact ?

5. **Les contraintes de l'environnement de recherche** : quelles sont les ressources à disposition : temps ? compétences ? budget ? accès aux terrains ? 

6. **La rigueur à appliquer** : s'agit-il d'une évaluation préliminaire ou d'une évaluation rigoureusement détaillée ?

7. **Pondération des critères précédents** : établir une pondération des six critères précédents en fonction de leur pertinence vis à vis du projet de recherche. Nous avons attribué à chacun des critères un coefficient allant de 1 (=critère non-pertinent) à 5 (=critère essentiel)

### Deuxième étape de construction de la stratégie d'évaluation : choix d'une stratégie
Une fois ces éléments identifiés, il s'agit de se reporter au Framework de Sélection des Stratégies d'Evaluation en *Design Science Research* (Venable et al., 2012, p.11) qui fait le lien entre ces critères et différentes stratégies possibles pour l'évaluation d'un artefact : évaluation *ex ante* ou *ex post*, [évaluation naturaliste ou artificielle](README.md). Chaque itération se voit alors attribuée un quadrant avec les caractéristiques de la(les) stratégie(s) choisie(s)

<table>
    <thead>
        <tr>
            <th colspan=2 rowspan=2>Framework de Selection des Stratégies d'Evaluation</th>
            <th>Ex ante (avant le prototypage) </th>
            <th>Ex post (après le prototypage) </th>
        </tr>
    </thead>
    <tbody>
      <tr>
            <td></td>
            <td></td>
            <td><ul><li>[ ] Evaluation formative</li><li>[ ] Cout moins élevé</li><li>[ ] Plus rapide</li><li>[ ] Evaluation du design, d'un protoype partiel ou complet</li><li>[ ] Moins de risque pour les participants durant l'évaluation</li><li>[ ] Risque plus élevé de faux positifs</li></ul></td>
            <td><ul><li>[ ] Evaluation sommative</li><li>[ ] Cout plus élevé</li><li>[ ] Moins rapide</li><li>[ ] Evaluation des instances</li><li>[ ] Plus de risques pour les participants durant l'évaluation</li><li>[ ] Risque moins élevé de faux positifs</li></ul></td>
        </tr>
        <tr>
            <td>Naturaliste</td>
            <td><ul><li>[ ] Plusieurs parties prenantes hétérogènes</li><li>[ ] Conflits substantiels</li><li>[ ] Artefacts socio-technique</li><li>[ ] Cout plus élevé</li><li>[ ] Evaluation longue</li><li>[ ] Accès au terrain requis</li><li>[ ] Evaluation de l'utilité de l'artefact</li><li>[ ] Niveau de rigueur : "On juge l'arbre à ses fruits"</li><li>[ ] Risque plus élevé pour les participants</li><li>[ ] Risque de faux positif plus faible</li></td>
            <td><ul><li>Utilisateurs réels, problèmes réels et système irréel</li><li>Cout faible-moyen</li></ul><ul><li>Vitesse correcte</li><li>Risque faible pour les participants</li><li>Risque élevé de faux positifs</li></ul></td>
            <td><ul><li>Utilisateurs réels, problèmes réels et systèmes réels</li><li>Cout le plus élevé</li><li>Risque le plus élevé pour les participants</li><li>Meilleure évaluation de l'utilité</li><li>Identification des effets collatéraux</li><li>Risque le moins élevé de faux positifs</li></ul></td>
        </tr>
        <tr>
            <td>Artificielle</td>
            <td><ul><li>[ ] Peu de parties prenantes similaires</li><li>[ ] Peu ou pas de conflits</li><li>[ ] Artefacts purement techniques</li><li>[ ] Cout moins élevé</li><li>[ ] Evaluation courte</li><li>[ ] Niveau de rigueur : contrôle des variables</li><li>[ ] Evaluation de l'efficacité </li><li>[ ] Risque moins élevé pendant l'évaluation</li><li>[ ] Risque de faux positif plus élevé</li></ul></td>
            <td><ul><li>Utilisateurs irréels, problèmes irréels et système irréels</li><li>Cout le moins élevé</li></ul><ul><li>Vitesse la plus élevée</li><li>Risque le moins élevé pour les participants</li><li>Risque le plus élevé de faux positifs</li></ul></td>
            <td><ul><li>Système réel, problème iréel et utilisateurs irréels</li><li>Cout moyen/élevé</li></ul><ul><li>Vitesse correcte</li><li>Risque faible-moyen pour les participants</li></ul></td>
        </tr>
    </tbody>
</table>


### Troisième étape de construction de la stratégie d'évaluation : choix des méthodes
A ce premier framework correspond un second, le *DSR Evaluation Method Selection Framework* qui fait correspondre à la stratégie choisie une liste de méthodes d'évaluations appropriéés. Il s'agit ensuite de choisir la (ou les) méthode(s) à appliquer à l'artefact. 

### Quatrième étape de construction de la stratégie d'évaluation : design du détail de l'évaluation 
Dans cette étape, il s'agit d'articuler les différentes stratégies et méthodes choisies de façon à former un design cohérent. Il s'agit également d'apprendre le fonctionnement des méthodes chosies et de les designer individuellement. 

### Cinquième étape de construction de la stratégie d'évaluation : réalisation des évaluations
Les méthodes choisies sont mises en place selon le design choisies. On collecte les données avant de les analyser et de tirer des conclusions qui visent à savoir dans quelle mesure l'artefact a répondu à la sous-question de recheche qui structure l'itération. 

### Sixièmé étape de construction de la stratégie d'évaluation : implication sur la prochaine itération
Dans cette dernière étape, on évalue la pertinence empirique et la rigueur scientifique de l'artefact avant de noter spécifiquement les manquements relatifs à ces deux aspects qui sont à améliorer dans la prochaine itération. 


- [ ] Terminer sur le "quand s'arrêter"
